# Как писать в файлы?

Содержание:
1. Уровни какие проходит запрос записи
2. Системный вызов/ОС
3. Файловая система
4. Диск
5. А как же рантайм
    - трансляция запросов
    - Рассказать про баг .NET 7
6. Выводы

# Введение

Приветствую. 

TODO: ... Про пет-проект ...

Данные приложения я храню на диске, в файле. Казалось бы, просто вызови `write` и дождись окончания записи.
Но немного исследовав эту тему, я понял, что не все так просто - существует большое количество подводных камней, которые надо учитывать, чтобы быть немного уверенным, что данные точно сохранены.

Для начала представим путь, который проходят данные, перед тем как быть записанными на диск:

TODO: блоки визуализации (Приложение -> ОС -> Файловая система -> Диск).

# Приложение

Зависит от языка какие вызовы нужно сделать
Есть буферризация
Привести примеры различных ЯП
А еще стандартную библиотеку C (как пример)
Ну и мой пример

Тут про fflush, Flush у BufferedStream (C#) и даже когда открывается файл, то внутри есть BufferedStrategy

Как защититься/что делать: зависит от ЯП

---
Все начинается с самого приложения. 
Обычно у нас имеется интерфейс для работы с файлами. Это зависит от ЯП, но примеры:
- `fwrite` - C
- `std::fstream.write` - C++
- `FileStream.Write` - C#
- `FileOutputStream.Write` - Java
- `open().write` - Python
- `File.Write` - GO

и другие.

Это все средства предоставляемые языками программирования, для работы с файлами: запись, чтение и т.д.
Их преимуществом является независимость от платформы, на которой мы работаем. 
Но также и привносит свои недостатки. 
В данном случае, это буферизация.

Судя по документации, то из приведенных выше все ЯП используют либо поддерживают буферизацию:
- C - [setvbuf](https://en.cppreference.com/w/c/io/setvbuf)
- C++ - [filebuf](https://cplusplus.com/reference/fstream/filebuf/)
- C# - [BufferedFileStrategy](https://github.com/dotnet/runtime/blob/main/src/libraries/System.Private.CoreLib/src/System/IO/Strategies/BufferedFileStreamStrategy.cs)
- Java - [Files.newBufferedReader](https://docs.oracle.com/javase/8/docs/api/java/nio/file/Files.html#newBufferedWriter-java.nio.file.Path-java.nio.charset.Charset-java.nio.file.OpenOption...-)
- Python - [io.BufferedIOBase](https://docs.python.org/3/library/io.html#io.BufferedIOBase)
- GO - [bufio.Reader](https://pkg.go.dev/bufio#Reader)

> Насчет C# - в реализации `FileStream` используется `FileStreamStrategy` класс, который обрабатывает запросы. 
> Например, при создании `FileStream` через `File.Open`, `BufferedFileStrategy` обертывает целевой `OSFileStreamStrategy`.

Вообще, буферизация в пространстве пользователя штука неплохая, так как позволяет повысить производительность. 
Но если не знать этого, то часть данных может быть не записана.
Тут может быть 2 случая:
1. Буферизованный файл создан вручную (GO, Java).
2. Буферизация происходит прозрачно для программиста (C, C++).

Если в первом случае мы точно знаем, что буферы надо сборосить после окончания записи, то второй вариант позволит выстрелить себе в ногу:
- Приложение экстренно закроется (например, получили SIGKILL нельзя обработать) и буферы уровня приложения просто не сбросятся.
- Файл после создания будет где-то в памяти и при закрытии буферы сброшены не будут, т.к. просто забудем сделать это.

Выходов здесь 2:
- Сбрасывать буферы после каждого сеанса записи. 
  Например, при логировании мы сначала всю пачку строк записываем через `write` и, только когда все были записаны сбрасываем буфер.
- Убрать буферизацию вообще и делать записи напрямую.

На мой взгляд, более привлекательный вариант - первый. 
Так как он позволит немного повысить производительность.

Для сравнения производительности я провел небольшой бенчмарк.

Записал последовательно в файл 64 Мб данных.
Тестирование производил на 2 машинах:
- Личный ноутбук: NVMe
- Старый сервер: HDD

Результаты следующие:

| Машина | Прямая запись, мс | Буферизированная запись, мс | 
|--------|-------------------|-----------------------------|
| Личный | 75.77             | 62.06                       |
| Старая | 122.1             | 104.9                       |

Как видно, разница заметна: время выполнения при буферизации меньше примерно в 1.2 раза.

<spoiler title="Код бенчмарка">

```cs
TODO: добавить код
```

</spoiler>

# ОС

Язык программирования дает хорошую абстракцию платформы - разработчику не нужно думать (как минимум, не так часто) на какой операционной системе работает приложение.
Но в любом случае функции языка будут транслироваться/превращаться в системные вызовы ОС, для записи в файлы.
Эти системные вызовы специфичны для каждой операционной системы, но в общем случае всегда есть для записи, чтения и открытия файлов.
Например, вот примерное отображение:

| Операция | *nix  | Windows   |
|----------|-------|-----------|
| Открытие | open  | OpenFile  |
| Чтение   | read  | ReadFile  |
| Запись   | write | WriteFile |
| Закрытие | close | CloseFile |

> Под *nix имеют Unix-подобные ОС (Linux, FreeBSD, OSX). 
> Название у этих вызовов одинаковые, хоть и поведение немного отличается.

На уровне ОС тоже присутствует буферизация - [страничный (дисковый) кэш](https://en.wikipedia.org/wiki/Page_cache).
И вот с помощью нее выстрелить в ногу еще проще. 

При работе с файлом (чтение/запись) данные из него читаются страницами, даже когда запрошен только 1 байт (записать или прочитать). 
Когда страница была изменена, то она называется "грязной" и будет сброшена на диск. 
Причем в памяти одновременно может находиться множество страниц, они и создают страничный кэш - буфер уровня ОС.

Так где это может нам повредить? Представим следующий процесс:
1. Нам пришел запрос на обновление данных о пользователе;
2. Мы открываем файл с данными;
3. Переписываем имеющийся диапазон имени переданным значением (представим, что для имени выделено в файле 255 символов);
4. Сообщаем пользователю, что имя успешно обновлено.

Где может возникнуть проблема? После 4 шага. Просто представим, что после отправки ответа пользователю об успешно выполненной операции произошло отключение электричества.
В результате имеем такую ситуацию:
- Пользователь думает, что имя успешно обновлено.
- Данные о пользователе хранятся старые.

Почему старые? Потому что перезаписанное имя хранилось на грязной странице в памяти, а не на диске, и при отключении электричества мы ее сохранить не успели.

Страничный кэш полезен, когда над одним участком памяти производится множество операций чтения/записи. 
Но в случае, когда изменения должны быть "закоммичены" нам нужно удостовериться, что записанные данные действительно были сброшены на диск.
Для этого можно применить несколько стратегий:
1. Системные вызовы для сброса буферов;
2. При открытии файла говорить сразу, что буферизация не нужна.

## Системные вызовы для сброса страниц 

Первый вариант использует специальные системные вызовы.
Для Linux можно использовать:
- `fdatasync(fd)` - проверяет, что данные в памяти и на диске синхронизированы, т.е. выполняет сброс страниц и при необходимости обновляет размер файла;
- `fsync(fd)` - то же самое что и `fdatasync`, но дополнительно синхронизирует метаданные файла (время доступа, изменения и др.);
- `sync_file_range(fd, range)` - проверяет, что указанный диапазон данных файла сброшен на диск;
- `sync()` - эта функция тоже синхронизирует содержимое буферов и дисков, только делает это для всех файлов, а не указанного.

Как уже было сказано, `fdatasync` синхронизирует только содержимое, без метаданных как `fsync`, поэтому он выполняется быстрее.


> Больше про эти системные вызовы описано в статье [Устойчивое хранение данных и файловые API Linux](https://habr.com/ru/companies/ruvds/articles/524172/).

Для Windows это:
- `_commit` - сбрасывает данные файла прямо на диск;
- `FlushFileBuffers(fd)` - вызывает запись всех буферных данных в файл (я не Windows разработчик, поэтому не знаю точно чем отличается от предыдущего вызова);
- `NtFlushBuffersFileEx(fd, params)` - сброс страниц на диск, но только для файловых систем NT (NTFS, ReFS, FAT, exFAT).

P.S. `NtFlushBuffersFileEx` [используется в Postgres](https://github.com/postgres/postgres/blob/874d817baa160ca7e68bee6ccc9fc1848c56e750/src/port/win32fdatasync.c#L40) как обертка для кроссплатформенного `fdatasync`.

## Говорим ОС, что синхронизация не нужна

Для второго варианта, нам необходимо открывать файл с необходимым параметром.

В Linux это можно сделать передав параметры `O_SYNC | O_DIRECT` функции `open` при открытии файла.
- `O_SYNC` говорит о том что `write` не должен возвращаться, пока данные не будут точно записаны на диск. Есть еще `O_DSYNC` - это про синхронизацию только данных, без метаданных;
- `O_DIRECT` говорит о том что для записи не нужно использовать страницы, т.е. запись будет происходить в обход страничного кэша.

Использование флага `O_SYNC`/`O_DSYNC` можно сравнить с тем, что после каждого write будет вызываться `fsync`/`fdatasync`, соответственно.

После открытия файла, с помощью `fcntl` можно изменить только `O_DIRECT` флаг, но не `O_SYNC`. Это прописано в описании `fcntl`:
> ... It is not possible to change the O_DSYNC and O_SYNC flags; see BUGS, below.

В Windows для этого есть свои аналоги: флаги `FILE_FLAG_NO_BUFFERING` и `FILE_FLAG_WRITE_THOUGH` для функции открытия файла `CreateFile`:
- `FILE_FLAG_NO_BUFFERING` - отключает буферизацию при записи. Аналог `O_DIRECT`; 
- `FILE_FLAG_WRITE_THOUGH` - каждая запись сразу сбрасывается на диск. Аналог `O_SYNC`;

В [документации](https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-createfilea#caching-behavior) дано описание поведению при указании обоих флагов:
> If FILE_FLAG_WRITE_THROUGH and FILE_FLAG_NO_BUFFERING are both specified, so that system caching is not in effect, then the data is immediately flushed to disk without going through the Windows system cache. The operating system also requests a write-through of the hard disk's local hardware cache to persistent media.

## Синхронизация директорий

Но это еще не все. 
Вспомним, что 1) файлы создаются, удаляются и перемещаются и 2) директории - тоже файлы. 
То есть при изменении содержимого директории, ее содержимое должно быть сброшено на диск так же как и файл.

В *nix выполняется это точно так же, как и с файлами - получаем дескриптор директории и выполняем `fsync(directory_fd)` на нее.
Это поведение также задокументированно:

> Calling  fsync() does not necessarily ensure that the entry in the directory containing the file has also reached disk. 
> For that an explicit fsync() on a file descriptor for the directory is also needed.

P.S. Я искал информацию о том, можно ли избежать ручного вызова `fsync` для директории через указание флага `O_SYNC`, но ничего не нашел. 
Если знаете, влияет ли этот флаг на работу с директориями, то подскажите в комментариях. 

Что же касается Windows, то там это сделать нельзя:
1. Директорию нельзя открыть - при попытке происходит ошибка `EACCESS`;
2. `FlushFileBuffers` работает только с файлами, либо томом (все файлы тома сбросить), но для последнего нужны повышенные привилегии.

Это поведение должно учитывать и в Postgres это тоже [проверяется](https://github.com/postgres/postgres/blob/874d817baa160ca7e68bee6ccc9fc1848c56e750/src/backend/storage/file/fd.c#L3797):
```c++
/*
 * fsync_fname_ext -- Try to fsync a file or directory
 *
 * If ignore_perm is true, ignore errors upon trying to open unreadable
 * files. Logs other errors at a caller-specified level.
 *
 * Returns 0 if the operation succeeded, -1 otherwise.
 */
int
fsync_fname_ext(const char *fname, bool isdir, bool ignore_perm, int elevel)
{
	/*
	 * Some OSs require directories to be opened read-only whereas other
	 * systems don't allow us to fsync files opened read-only; so we need both
	 * cases here.  Using O_RDWR will cause us to fail to fsync files that are
	 * not writable by our userid, but we assume that's OK.
	 */
	flags = PG_BINARY;
	if (!isdir)
		flags |= O_RDWR;
	else
		flags |= O_RDONLY;

	/*
	 * Some OSs don't allow us to open directories at all (Windows returns
	 * EACCES), just ignore the error in that case.  If desired also silently
	 * ignoring errors about unreadable files. Log others.
	 */
	if (fd < 0 && isdir && (errno == EISDIR || errno == EACCES))
		return 0;
	else if (fd < 0 && ignore_perm && errno == EACCES)
		return 0;
	else if (fd < 0)
	{
		ereport(elevel,
				(errcode_for_file_access(),
				 errmsg("could not open file \"%s\": %m", fname)));
		return -1;
	}

	returncode = pg_fsync(fd);

	/*
	 * Some OSes don't allow us to fsync directories at all, so we can ignore
	 * those errors. Anything else needs to be logged.
	 */
	if (returncode != 0 && !(isdir && (errno == EBADF || errno == EINVAL)))
	{
		// ...
		return -1;
	}

	return 0;
}
```

## Ошибки fsync

`fsync` _может_ вернуть ошибку и ее необходимо обработать. 
Это было показано в примере выше.
Если она вернула ошибку, то единственное допустимое действие - завершение работы.
Почему? 
Потому, что после этого мы **не можем быть уверены** в том, что сам **файл остался в согласованном состоянии**, даже если с точки зрения файловой системы он не поломан (об этом будет дальше),
- Файловая система может быть повреждена;
- В файле может появиться дыра (неправильная запись);
- Грязные страницы для записи могут быть помечены чистыми и больше сброшены не будут. 

Последнее может привести к тому, что файл на диске и в памяти имеют разное содержимое, но заметно этого не будет. 

Для справедливости стоит сказать, что пометка страниц чистыми - это часть реализации Linux, так как он предполагает, что файловая система возьмет на себя обязательства корректно закончить операции. 
Вот [тут](https://wiki.postgresql.org/wiki/Fsync_Errors#Research_notes_and_OS_differences) есть примеры поведения различных ОС при ошибке `fsync` (что происходит со страницами):
- Darwin/macOS - отбрасываются;
- OpenBSD - отбрасываются;
- NetBSD - отбрасываются;
- FreeBSD - остаются грязными;
- Linux (после 4.16) - помечаются чистыми;
- Windows - неизвестно.

Можно привести пример, когда некорректное управление этими ошибками приводило к потерям данных - сохранение данных WAL в Postgres.
Изначально, разработчики базы данных предполагали, что семантика `fsync` следующая: 

> Если `fsync()` выполнился успешно, то все записи с момента последнего _успешного_ `fsync` были сброшены на диск.

Т.е. если мы сейчас вызвали `fsync` и он вернул ошибку, то мы можем просто повторить этот вызов в будующем в надежде, что данные в итоге попадут на диск.
Но это ошибочное предположение. 
В реальности, если `fsync` вернул ошибку, то грязные страницы будут просто "забыты", т.е. семантика:

> Если `fsync()` выполнился успешно, то все записи с момента последнего ~~успешного~~ `fsync` были сброшены на диск.

Т.е. все ошибки синхронизации данных с диском просто игнорировались. 
Это было замечено в 2018 году и вопрос поднялся в [списке рассылки](https://lwn.net/Articles/752093/).
Багу даже дали название `fsyncgate 2018` и посвятили отдельную страницу на [вики](https://wiki.postgresql.org/wiki/Fsync_Errors).
Сам баг исправлен в версии 12 (и во многих предыдущих) в [этом коммите](https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=9ccdd7f66e3324d2b6d3dec282cfa9ff084083f1) и если быть точным, то исправили следующим образом:

```c++
// fd.c
int
data_sync_elevel(int elevel)
{
    return data_sync_retry ? elevel : PANIC;
}

// Любая функция
void sample_function() 
{
    // Любой вызов fsync в логике 
    if (pg_fsync(fd) != 0)
    // ereport(ERROR,
       ereport(data_sync_elevel(ERROR),
                (errcode_for_file_access(),
                 errmsg("could not fsync file \"%s\": %m", path)));
}
```

<spoiler title="Страничный кэш в БД">

Работа с диском - важная часть любой базы данных. Поэтому многие реализуют свою систему работы со страничным кэшэм и не полагаются на механизмы ОС.
Благодаря этому:
- Более эффективный IO, т.к. запись на диск производится только после `COMMIT`; 
- Используются (потенциально) более оптимальные алгоритмы замещения страниц;
- Размер страницы и их количество в памяти может настраиваться;
- Безопасность работы с данными;

Вот примеры некоторых СУБД:

| СУБД           | Реализация                                                                                                                                                                                                                                                             | Где почитать                                                                                                                                 | Алгоритм замещения страниц |
|----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|
| Postgres       | [bufmgr.h](https://github.com/postgres/postgres/blob/d13ff82319ccaacb04d77b77a010ea7a1717564f/src/include/storage/bufmgr.h), [bufmgr.c](https://github.com/postgres/postgres/blob/d13ff82319ccaacb04d77b77a010ea7a1717564f/src/backend/storage/buffer/bufmgr.c)        | [WAL в PostgreSQL: 1. Буферный кеш](https://habr.com/ru/companies/postgrespro/articles/458186/)                                              | clock-sweep                |
| SQL Server     | Исходников не нашел                                                                                                                                                                                                                                                    | [Обзор компонентов управления памятью в SQL Server](https://habr.com/ru/articles/233365/)                                                    | LRU-2 (LRU-K)              |
| Oracle         | Исходников не нашел                                                                                                                                                                                                                                                    | [Oracle Memory Architecture](https://www.appservgrid.com/documentation111/docs/rdbms10g/windows/doc/server.101/b10743/memory.htm#sthref1252) | LRU, Temperature-based     |
| MySQL (InnoDB) | [buf0buf.h](https://github.com/mysql/mysql-server/blob/824e2b4064053f7daf17d7f3f84b7a3ed92e5fb4/storage/innobase/include/buf0buf.h), [buf0buf.cc](https://github.com/mysql/mysql-server/blob/824e2b4064053f7daf17d7f3f84b7a3ed92e5fb4/storage/innobase/buf/buf0buf.cc) | [InnoDB Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html)                                                        | LRU                        |

Как уже было сказано, собственный менеджер буферов позволяет оптимизировать работу СУБД, поэтому многие разработчики не останавливаются на единственном "умном" алгоритме замещения страниц. Примеры:
- ORACLE и SQL Server имеет возможность использовать flash накопители в качестве временного хранения буферов, вместо сброса на основной диск (параметр `DB_FLASH_CACHE_FILE` для Oracle и расширение Buffer Pool для SQL Server);
- Postgres позволяет "разогревать" кэш страниц с помощью расширения `pg_prewarm`;

</spoiler>

### TODO: возможный код реализации fsync - https://github.com/torvalds/linux/blob/67be068d31d423b857ffd8c34dbcc093f8dfff76/fs/buffer.c#L769

# Файловая система

Когда речь идет про запись в файлы, обычно говорят "записать на диск". 
Отчасти это верно, но между ОС и диском есть важное связующее звено - файловая система.
Она отвечает за то, как и куда будет производиться запись.

Файловых систем существует [огромное количество](https://en.wikipedia.org/wiki/Comparison_of_file_systems), поэтому я выделю лишь часть:
- ext4, ext3, ext2
- btrfs
- xfs
- ntfs

> Выбрал эти, так как часто упоминаются и многие исследования используют именно их.

Каждая файловая система обладает своими характеристиками, параметрами и особенностями (например, максимальная длина имени файла), но сейчас нас интересуют те, что связаны с записью и сохранностью данных.

## Поддержка целостности файловой системы

Для обеспечения целостности файловой системы могут использоваться различные механизмы:
- Журналирование (ext3, ext4, ntfs, xfs) - файловая система ведет лог операций (WAL)
- Copy-on-write (btrfs, zfs) - при пере/до записи содержимое не меняется, а вместо этого выделяется новый блок, куда новые данные и записываются
- log structured - сама файловая система является большим логом

Но не все файловые системы поддерживают подобные механизмы безопасности. 
Например, ext2 - не журналируемая и любые операции идут сразу в файл.

Вроде бы вот таблетка от проблем - выбирай файловую систему с журналом (или другим механизмом) и радуйся жизни.
Но нет.

Исследование [Model-Based Failure Analysis of Journaling File Systems](https://research.cs.wisc.edu/wind/Publications/sfa-dsn05.pdf) показало, что даже журналируемые файловые системы могут оставить систему в некорректном состоянии.
Было изучено поведение 3 файловых систем (ext3, reiserfs и jfs) при возникновении ошибки записи блока.
Результат их работы приведен в следующей таблице. 


### TODO: таблица

Можно заметить, что каждая из этих файловых систем может оставить содержимое файла (Data Block) в некорректном состоянии (DC, Data Corruption).
То есть, на журналирование полагаться не стоит.

Про NTFS тоже нашел [исследование](https://pages.cs.wisc.edu/~laksh/research/Bairavasundaram-ThesisWithFront.pdf) (страница 163) - даже с учетом дублирования/репликации метаданных файловая система может быть повреждена и не подлежать восстановлению.

А что с другими механизмами - copy-on-write и log structured?

В [этом](https://elinux.org/images/b/b6/EMMC-SSD_File_System_Tuning_Methodology_v1.0.pdf) исследовании (результат на 26 странице) тестировалась btrfs на SSD и после нескольких отключений питания файловая система пришла в негодность и не смогла восстановиться, хотя журналируемый ext4 пережил 1406.  

### TODO: может в отдельную секцию вынести

Также сами разработчики могут ошибиться в реализации - версии 5.2 и 5.3 btrfs [не рекомендуется](https://bugs.archlinux.org/task/63733) использовать.

Про log structured файловые системы исследований найти не смог.

## Про запись в файл

Обобщая, файл в файловой системе представляется 2 компонентами - метаданные и блоки данных.
При записи в файл надо задать вопрос - что обновлять вначале?

Если сначала обновить метаданные, то после их обновления, в файле может появиться мусор:
1. Делаем запрос на дозапись в файл
2. Файловая система обновляет метаданные - увеличивает длину файла
3. Происходит отказ.

В результате мы имеем файл, который заполнен мусором, хотя ни файловая система, ни диск проблем не видят.

Защититься от этого можно если выставлять специальные маркеры в файле, которые бы нам говорили, что файл инициализирован корректно и все хорошо.

Например, можно использовать константу, чтобы проверять, что дальше действительно идут наши данные, а не мусор.
Такой подход используют:
- Postgres в начале заголовка страницы WAL выставляет magic number 
```c++
/*
 * Each page of XLOG file has a header like this:
 */
#define XLOG_PAGE_MAGIC 0xD114	/* can be used as WAL version indicator */

typedef struct XLogPageHeaderData
{
	uint16		xlp_magic;		/* magic value for correctness checks */
	// ...
} XLogPageHeaderData;
```
- Kafka [использует magic number](https://github.com/apache/kafka/blob/9bc9fae9425e4dac64ef078cd3a4e7e6e09cc45a/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java#L63) в качестве номера версии
```java
public class FileLogInputStream implements LogInputStream<FileLogInputStream.FileChannelRecordBatch> {
    @Override
    public FileChannelRecordBatch nextBatch() throws IOException {
        // ...
        
        byte magic = logHeaderBuffer.get(MAGIC_OFFSET);
        final FileChannelRecordBatch batch;

        if (magic < RecordBatch.MAGIC_VALUE_V2)
            batch = new LegacyFileChannelRecordBatch(offset, magic, fileRecords, position, size);
        else
            batch = new DefaultFileChannelRecordBatch(offset, magic, fileRecords, position, size);
        return batch;
    }
}
```
- А может быть не числом, а строкой, как в SQLite, причем как [осмысленной](https://github.com/sqlite/sqlite/blob/f79b0bdcbfb46164cfd665d256f2862bf3f42a7c/src/btree.c#L3267), так и [случайной](https://github.com/sqlite/sqlite/blob/f79b0bdcbfb46164cfd665d256f2862bf3f42a7c/src/pager.c#L1608)
```c++
// Заголовок журнала отката - несмысленная константа
/*
** Journal files begin with the following magic string.  The data
** was obtained from /dev/random.  It is used only as a sanity check.
*/
static const unsigned char aJournalMagic[] = {
  0xd9, 0xd5, 0x05, 0xf9, 0x20, 0xa1, 0x63, 0xd7,
};

static int readJournalHdr(
  Pager *pPager,               /* Pager object */
  int isHot,
  i64 journalSize,             /* Size of the open journal file in bytes */
  u32 *pNRec,                  /* OUT: Value read from the nRec field */
  u32 *pDbSize                 /* OUT: Value of original database size field */
){
  int rc;                      /* Return code */
  unsigned char aMagic[8];     /* A buffer to hold the magic header */
  i64 iHdrOff;                 /* Offset of journal header being read */

  /* Read in the first 8 bytes of the journal header. If they do not match
  ** the  magic string found at the start of each journal header, return
  ** SQLITE_DONE. If an IO error occurs, return an error code. Otherwise,
  ** proceed.
  */
  if( isHot || iHdrOff!=pPager->journalHdr ){
    rc = sqlite3OsRead(pPager->jfd, aMagic, sizeof(aMagic), iHdrOff);
    if( rc ){
      return rc;
    }
    if( memcmp(aMagic, aJournalMagic, sizeof(aMagic))!=0 ){
      return SQLITE_DONE;
    }
  }
  
  // ...
}

// Заголовок файла БД - человекочитаемая строка
#ifndef SQLITE_FILE_HEADER /* 123456789 123456 */
#  define SQLITE_FILE_HEADER "SQLite format 3"
#endif

/*
** The header string that appears at the beginning of every
** SQLite database.
*/
static const char zMagicHeader[] = SQLITE_FILE_HEADER;

static int lockBtree(BtShared *pBt){
  // ...
  if( nPage>0 ){
    /* EVIDENCE-OF: R-43737-39999 Every valid SQLite database file begins
    ** with the following 16 bytes (in hex): 53 51 4c 69 74 65 20 66 6f 72 6d
    ** 61 74 20 33 00. */
    if( memcmp(page1, zMagicHeader, 16)!=0 ){
      goto page1_init_failed;
    }
    
// ...

page1_init_failed:
  pBt->pPage1 = 0;
  return rc;
}
```

Дополнительно можно выделить заголовки начала файлов разных форматов, например, BOM, JPEG, PNG.

Но это константа, которая не зависит от записанных данных. 
Что будет, если мы успешно запишем ее, а потом в середине операции произойдет отказ? 
Часть данных будет записана, а другая нет. 
Причем мы не можем сказать, какая именно часть была записана успешно - может запись началась с последней 

Для подобных ситуаций используют чек-суммы. 
Их можно записать как в начале, так и в конце.
Я считаю, что лучше записывать чек-сумму в конце:
- Меньше потенциальных позиционирований головки диска
- Записать все можно за 1 проход по данным
- Не будет лишних загрузок страниц с диска (даже если записываем 4 байта, нужно загружать всю страницу)


Таким маркером может быть чек-сумма.
Этот подход используют многие приложения.


Для примера:
- Postgres [использует CRC](https://github.com/postgres/postgres/blob/5f79cb7629a4ce6321f509694ebf475a931608b6/src/include/access/xlogrecord.h#L49) для проверки целостности записей WAL
```c++
typedef struct XLogRecord
{
    // ...
	pg_crc32c	xl_crc;			/* CRC for this record */
} XLogRecord;
```
- etcd использует скользящую чек-сумму для записей WAL  
```go


```

На этот счет у разных файловых систем есть разные мнения

здесь про обновление метаданных, а потом/до данных
про чек суммы сказать

### TODO: сначала модель согласованности, а потом про чек-суммы

## Модель согласованности

Прежде чем идти вперед, стоит поговорить о модели согласованности.

Есть такое понятие как модель памяти. 
В разных контекстах я находил разные определения, но скажем, что модель памяти - это правила, которые могут применяться для переупорядочивания store/load операций, т.е. их нарушение запрещается.
Для примера у нас есть следующий участок кода:
```c++
void function() {
    // 1
    a = 123;
    b = 244;

    // 2
    int c = a;
    b = 555; 
}
```

Вопросы такие:
- В 1 случае, можно ли сначала записать `b` и только потом `a`, т.е. изменить порядок store/store операций.
- Во 2 случае, можно ли сначала записать значение `b` и только потом прочитать значение из `a`. 

На такие вопросы отвечает модель памяти.
Для ЯП и железа есть документы с описанием их модели памяти: 
- [C++](https://en.cppreference.com/w/cpp/language/memory_model) 
- [C#](https://github.com/dotnet/runtime/blob/main/docs/design/specs/Memory-model.md)
- [Java](https://www.cs.umd.edu/~pugh/java/memoryModel/jsr133.pdf) 
- [Python](https://peps.python.org/pep-0583/)
- [Go](https://go.dev/ref/mem)
- [Rust](https://doc.rust-lang.org/nomicon/atomics.html
- [AMD64](https://www.amd.com/content/dam/amd/en/documents/processor-tech-docs/programmer-references/24592.pdf)

Но оставим ЯП на потом. 
Сейчас важно понять, что для файловых систем тоже можно определить подобные правила переупорядочивания.

В исследовании [Specifying and Checking File System Crash-Consistency Models](https://www.cs.utexas.edu/~bornholt/papers/ferrite-asplos16.pdf) подобное было названо Crash-Consistency Model - модель согласованности при сбоях. 
Дальше буду использовать это понятие.

В исследовании [All File Systems Are Not Created Equal](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-pillai.pdf) выделили следующие операции:
- Перезапись чанка файла
- Дозапись в файл
- Переименование
- Операции с директориями

После проведения экспериментов с ext2, ext3, ext4, reiserfs, xfs и btrfs была составлена эта таблица:

### TODO: скриншот таблицы
![Свойства файловых систем]()

> Стоит сделать замечание, что таблица показывает _найденные воспроизводимые_ нарушения. 
> Т.е. в реальности другие файловые системы тоже могут переупорядочивать операции.

Для простоты дальше я буду называть журналируемыми файловыми системами те, у которых есть какой-либо условный буфер, в который попадают операции прежде чем примениться.
Это могут быть soft updates, log-structured файловая система, COW.

### Атомарность 

При операциях с файловой системой часто необходимо обновлять сразу несколько мест и во время их обновлений может оказаться в некорректном состоянии.
Например, при дозаписи в файл требуется обновить длину в его inode, создать и инициализировать новый блок данных.

Атомарность, в данном случае, означает атомарность всех этих операций. 

Из таблицы можно сделать следующие выводы:
- Никакая файловая система не может атомарно дозаписать несколько блоков, разве что один
- Перезапись 1 сектора практически везде атомарна
- Нежурналируемые файловые системы почти всегда не предоставляют атомарность операций
- Операции с директориями почти всегда атомарны, за исключением файловых систем вообще без журнала (еще и ext2, но чувствую, что он не так часто используется)

Что может случиться, если произойдет отказ во время операции, которая не является атомарной?
Самое простое - нарушится целостность и придется запускать `fsck` (который не всегда может все восстановить).

В случае, если во время неатомарной операции произойдет сбой:
- В файле окажется мусор, если производилась дозапись - выделили новый блок данных, но ничего не записали, либо записали частично
- Перезапишется только часть данных - операция перезаписи не завершилась до конца

Также и в исследовании [Specifying and Checking File System Crash-Consistency Models](https://www.cs.utexas.edu/~bornholt/papers/ferrite-asplos16.pdf) было изучено поведение нескольких файловых систем в случае отказа.
Была построена следующая таблица:
### TODO: сравнение файловых систем

Закрашенные точки показывают, что какое-то поведение могло привести к нарушению целостности. 
В частности в случае отказа:
- `PA` (Prefix Append, Safe Append) - безопасное добавление не гарантируется всем файловыми системами, т.е. может появиться мусор
- `ARVR` (Atomic Replace Via Rename) - атомарное обновление содержимого существующего файла через его переименование не всеми гарантируется, т.е. при замене старого файла на новый, в файле может оказаться только часть данных
- `ACVE` (Atomic Create Via Rename) - атомарное создание нового файла через переименование никем не гарантируется, т.е. при переименовании временного файла, новый может не содержать всех данных и быть просто нулевой длины

### Переупорядочивание

Касательно переупорядочивания можно сделать следующий вывод: если файловая система журналируемая, то порядок операций (в большинстве случаев) сохраняется.

Этого нет, например, в ext2, ext3-writeback, ext4-writeback, reiserfs-nolog, reiserfs-writeback и поэтому операции могут спокойно быть переупорядочены.

Также важным является факт переупорядочивания операций над директориями с другими. 
Это означает, что если мы сначала записываем файл, а потом хотим его переименовать, то может случиться так, что мы сначала переименуем файл, (т.е. потенциально старое содержимое потеряем) и только потом начнем запись. 
К моему удивлению, на такое переупорядочивание способны не все, даже некоторые нежурналируемые могут сохранять порядок (xfs не ожидал его здесь увидеть).

### Барьер записи

В модели памяти, есть такое понятие как барьер записи - механизм, который запрещает переупорядочивание таких store/load последовательностей.
Для модели файловой системы он тоже есть - это `fsync()`.

Можно сказать, что этот барьер имеет следующую семантику - все вызовы записи до `fsync()` сброшены на диск. 
Мы не можем сказать, в какой последовательности они были выполнены, но _можем_ утверждать, что они были выполнены, а до этого момента, состояние просто неопределеное.

Вообще, `fsync()` - не барьер записи, просто его можно так использовать. 
Подобная тема уже поднималась - было предложение добавить новый системный вызов `fbarrier()`, который им бы и являлся, но Линус идею отверг, посчитав, что это лишняя трата времени.
### TODO: ссылка на высказывание линуса про ненужность fbarrier

## Примеры проблем

В том же исследовании All File Systems Are Not Created Equal были анализированы несколько приложений на наличие уязвимостей, касательно моделей согласованности в случае отказа.

В этой таблице описание алгоритмов (порядка вызовов) при работе с диском:
### TODO: таблица с протоколами

А результаты анализа приведены в следующей таблице:

### TODO: таблица с результатами анализа 

Какие из нее можно сделать выводы:
- Приложения, в которых работа с данными критична (СУБД например), часто используют `fsync` в качестве барьера
- Чем хуже предоставляемые файловой системой гарантии касательно атомарности и переупорядочивания - тем больше багов можно обнаружить (и нарушений целостности)
- Каждое приложение имеет свои представления о гарантиях файловых систем и при их нарушении может пострадать целостность. Например, здесь приведены [предположения SQLite](https://sqlite.org/atomiccommit.html#_hardware_assumptions) и сами авторы исследования обнаружили, что ZooKeeper требует атомарности записи в файл лога.

## Другие файловые системы

Исследования выше были ориентированы на файловые системы *nix мира, которые должны быть независимы, но существуют и другие, о который пойдет речь.

### NTFS

NTFS - "стандартная" файловая система для Windows. 
Я не нашел исследований, касательно ее отказоустойчивости, но, опираясь на ее описание, можно сделать следующие выводы:
1. Журналируются только метаданные, т.е. в файлах может появиться мусор
2. Имеет свой транзакционный API, но разработчикам [рекомендуется искать](_https://learn.microsoft.com/en-us/windows/win32/fileio/deprecation-of-txf#abstract) ему альтернативы

### APFS

APFS (Apple File System) - файловая система для Apple, которая должна заменить HFS+.
Из статей выделил следующее:
1. Вместо журналирования используется Copy On Write, причем новая технология `novel copy-on-write metadata scheme` (информацию про нее не нашел)
2. Имеет технологию `Atomic Safe-Save` для гарантии атомарного `rename`
3. Использует чек-суммы только для собственных метаданных, но не данных пользователей

Последний пункт я выделил специально, так как я неправильно понял [статью](https://danluu.com/filesystem-errors#error-detection), на которую опирался.
В ней прописано `apfs doesn’t checksum data because “[apfs] engineers contend that Apple devices basically don’t return bogus data”`, но если перейти на ссылаемую статью, то там прописано `APFS checksums its own metadata but not user data`.
Недопонимание произошло из-за того, что в изначальной статье идет сравнение с ZFS (в которой реализованы чек-суммы данных пользователя), а в той, что изучал я - про нее ни слова.

## Ошибки fsync

В предыдущей главе было сказано, что от ошибок `fsync` не восстановиться и описано как различные ОС обрабатывают подобные ситуации.
Но было сказано только про пометку страниц в памяти.

Данные я взял из исследования [Can Applications Recover from fsync Failures?](https://www.usenix.org/system/files/atc20-rebello.pdf). 
Оно говорит само за себя - в нем было проведено исследование поведения различных приложений, файловых систем и ОС в случае ошибки `fsync`.

Для начала посмотрим на таблицу с результатами тестирования файловых систем в случае ошибки `fsync`.

### TODO: таблица

> ext4 data - означает journal режим

Из этой таблицы можно сделать следующие выводы:
- Ошибки `fsync` возникают только в случае проблем с записью блоков данных или журнала, т.к. метаданные сначала журналируются. 
  Но при этом, если ошибка в метаданных будет найдена, то XFS и BTRS станут недоступны для записи (фс либо закроется (XFS), либо перемонтируется в ro режиме соответственно), а ext4 просто залогирует это и продолжит работу.
- При ошибке записи в блоки данных (добавление новых блоков, перезапись), метаданные _могут_ быть обновлены, т.е. размер файла изменится, хотя данные останутся старыми/создастся дыра или мусор.
- Если произойдет ошибка во время записи и файловую систему удастся восстановить, то состояние файла на диске и в ОС может различаться. В примере это btrfs - при ошибке записи, метаданные не меняются, но в памяти сохраняется старый дескриптор файла, который указывает на позицию в файле за его пределами (при записи).
- Все файловые системы помечают страницы чистыми, но это связано с тем, что тесты производились на Linux - в других системах поведение может отличаться (примеры были выше).
- Ошибка `fsync` не обязательно возвращает ошибку, если запись провалилась _сейчас_ - ext4 в data режиме может вернуть ошибку только при следующем вызове, то есть может произойти такая ситуация: 
     1. Записываем данные
     2. `fsync` возвращает успешный статус код
     3. Очищаем буфер с данными для записи
     4. Записываем новую порцию данных
     5. `fsync` возвращает ошибку
     В этом случае, мы уже не можем просто повторить операцию, так как старые данные мы _не сохранили_, а повтор текущей операции может _сохранить данные, которые полагаются на те, что пропали_.
- Не все файловые системы при ошибке перемонтируются в ro режиме - т.е. даже при ошибке `fsync` мы не гарантируем того, что можем приложение на этом узле запустить в read-only режиме.
- ext4 в journal режиме сохраняет целостность только для самой файловой системы, но с точки зрения приложения/пользователя, данные могут быть в некорректном состоянии.

Можно сказать, что работая с файловой системой можно ожидать любого поведения (любая комбинация из возможных поведений как в этой таблице).

Также было исследовано поведение различных приложений в случае ошибки `fsync`.

Результаты приведены в этой таблице

### TODO: таблица

> OV (old value) - возвращение старого значения, а не нового
> FF (false failure) - пользователю говорим, что операция провалилась, но новое значение сохранено
> KC/VC (key/value corruption) - данные были повреждены (тесты на key-value хранилище проводились)
> KNF (key not found) - пользователю говорим, что операция выполнилась, но новое значение не сохранилось на диске (пропало)

Можно сделать следующие выводы:
- Если ошибка `fsync` не проявляется сразу (ext4 journal режим), то ошибок возникает гораздо больше.
- COW файловые системы (на примере btrfs) лучше справляются с ошибками `fsync`, чем обычные журналируемые.
- В случае обнаружения ошибки многие приложения просто останавливаются и откатываются до последнего корректного состояния (`-`, `|` в ячейках таблицы).
- Приложения больше нацеливаются на конкретные ОС, а не файловые системы - поэтому так много различий в поведении для разных файловых систем.

> Интересное замечание авторов - Redis не обращает внимание на код ошибки fsync и всегда возвращает успешный результат. 
> Скорее всего, это потому что Redis в первую очередь In-Memory БД и сохранение данных просто небольшая фича.

# Хранилище

Вот последний этап - постоянное хранилище данных.
Сейчас мы говорим про:
- Жесткие диски, HDD
- Твердотельные накопители, SSD

<spoiler title="Не только HDD/SSD">

Кроме HDD и SSD стоило бы указать еще:
- Леточные накопители
- CD/DVD/Blu-ray диски
- PCM, FRAM, MRAM

Дальше про них разговор вестись не будет, но не хорошо вот так их опустить.

Ленточные носители - хороший вариант для резервного копирования. 
Если верить [этой статье](https://habr.com/ru/companies/x-com/articles/667542/), то они:
1. Самые надежные - 15-30 лет
2. Вмещают много - 18 Тб
3. Мало стоят - 1300 р/Тб (за пример взял [этот картридж](https://ltoshop.ru/lentochnyy_nositeli_kartridzhi/lto_9_ultrium_1845tb/lentochnyy_nositel_dannyh_hpe_lto_9_ultrium_1845tb_worm_q2079w.html)))

CD/DVD/Blu-ray диски продолжают использоваться. 
Судя по [анализу рынка в США](https://www.anythingresearch.com/industry/Manufacturing-Reproducing-Magnetic-Optical-Media.htm) их оборот только увеличивается.
Скорее они подходят для распространения контента (игры, фильмы, музыка), чем для интенсивных IO операций, поэтому тоже пропустим.

Также за бортом я оставил такие технологии как [PCM](https://en.wikipedia.org/wiki/Phase-change_memory) (Phase-change Memory), [FRAM](https://ru.wikipedia.org/wiki/FRAM) (Ferroelectic RAM) и [MRAM](https://en.wikipedia.org/wiki/Magnetoresistive_RAM) (Magnetoresistive RAM).
Данных по ним я не нашел.

</spoiler>

Механизмы хранения, которые они используют, различаются, но сейчас это не главное. 
Главное то, что мы можем выделить параметры, которыми их можно охарактеризовать:
- Износ
- ECC
- Контроллер доступа

## Износ

Каждое оборудование изнашивается.
И если железо (процессор, видеокарта) испортится, то мы это быстро заметим и заменим. 
В общем, ничего страшного не случиться.

Но если умрет хранилище, то мы можем потерять данные.
Blackbaze выпустили [отчет за 2023](https://www.backblaze.com/blog/backblaze-drive-stats-for-2023/) год по статистике отказов своих жестких дисков.
Из него можно сделать следующие выводы:
- [Среднее время жизни HDD](https://www.backblaze.com/blog/wp-content/uploads/2024/02/4-Lifetime-AFR.png) зависит от многих факторов, например, производителя или размера диска, но если вычислить среднее по больнице, то это примерно 65 месяцев (5 лет и 5 месяцев)
- В сравнении с 2022 годом AFR (Annualized Failure Rate, вероятность отказа в году) дисков в среднем увеличился. Это было замечено еще  

Что касается SSD, но у них тоже есть [отчет, но за 2022 год](https://www.backblaze.com/blog/ssd-drive-stats-mid-2022-review/). 
Согласно ему, AFR для SSD - 0.92% (ниже чем у HDD).
Но стоит учитывать, что в эксплуатацию SSD взяли только в 2018 году, поэтому статистику надо еще собрать.

Также стоит поговорить о влиянии физического мира на накопители.
- В HDD больше движущихся деталей, поэтому он сильно подвержен физическим воздействиям. 
  За примерами ходить не надо - [Shouting in the Datacenter](https://www.youtube.com/watch?v=tDacjrSCeq4).
  На этом видео было показано как сильно возрастает задержка ответа HDD, когда на него кричат.
  Но это не единственное - в [этом исследовании](https://www.princeton.edu/~pmittal/publications/acoustic-ashes18.pdf) провели анализ влияния шумов на работу HDD.
  Грубо говоря, HDD заставили работать при постоянных шумах (устроили ему ADoS, Acoustic Denial of Service)  и в результате возросло количество ошибок позиционирования, а в не которых случаях и отказу всего диска.
- Хоть в SSD и нет подобных движущихся деталей, но зато есть электричество. 
  И вот к нему, а точнее его внезапному отключению, он уязвим.
  В [этом исследовании](https://arxiv.org/pdf/1805.00140.pdf) провели тестирование поведения SSD на внезапное отключение электричества во время работы. 
  И, _внезапно_, отключение электричества приводит к нарушению целостности данных, их потере, либо вообще [может превратить SSD](https://www.usenix.org/system/files/conference/fast13/fast13-final80.pdf) в кирпич.

## ECC

HDD и SSD имеют встроенную поддержку ECC - Error Correction Code, Коды Исправления Ошибок:
- HDD имеет поддержку разметки секторов согласно [Advanced Format](https://en.wikipedia.org/wiki/Advanced_Format), который позволяет хранить ECC для всего сектора (для этого нужна дополнительная поддержка со стороный ОС, которая есть сегодня практически везде, но в случае старых систем может не быть)
- SSD тоже имеет подобную поддержку, но только для NAND технологии (используется в "быту" - обычные SSD, USB флешки, SD карты), но в NOR не так часто (используется в микроконтроллерах и в ситуациях, когда обновления происходят редко), т.к. последний по своему устройству более надежен. 

Стоит учитывать еще и то, что файловые системы знают [про Advanced Format и могут под него подстраиваться](https://wiki.archlinux.org/title/Advanced_Format#File_systems), но это за рамками статьи.

Но даже если ECC и есть, то он не всегда может справиться с ошибками. 
В [этом исследовании](https://arxiv.org/pdf/2012.12373.pdf) сравнивали жизненные циклы HDD и SSD путем тестирования их на собственных датасетах.
Эти графики показывают сколько было обнаружено неисправимых ошибок (UE, Uncorrectable Error) до момента отказа диска, т.е. ошибок, которые не могли исправить ECC

### TODO: таблица 12 с UE 

Выводы можно сделать следующие:
- Количество неисправленных ошибок у SSD зависит от времени жизни диска, тогда HDD - от времени позиционирования головки диска (Head Flying Hours)
- Перед отказом количество ошибок HDD резко возрастает за 2 дня
- Количество неисправленных на SSD больше, чем для HDD

Главный вывод - ECC иногда может не справляться с нарушением целостности.

В итоге, можно сказать, что чаще всего (современное оборудование и версии ОС) накопители имеют коды коррекции ошибок, но полностью полагаться на них не стоит.

## Контроллер доступа

Последний элемент, который стоит рассмотреть - это контроллер накопителя.
Этот компонент знает как работать с физическим хранилищем и обрабатывает приходящие от ОС запросы.
Важной деталью здесь является дисковый кэш - помимо страничного буфера есть еще и дисковый кэш.
Туда попадают все модифицирующие операции.

Вспомним про `fsync` - он должен обеспечивать сброс всех данных на диск, т.е. возвращается только когда данные уже на диске.
Если посмотреть man для `fsync` сейчас, то можно увидеть:

> The fsync() implementations in older kernels and lesser used filesystems do not know how to flush disk caches.
> In these cases disk caches need to be disabled using hdparm(8) or sdparm(8) to guarantee safe operation. 

Т.е. в старых версиях ядра (судя по указанной ранее версии 2.2) `fsync` не знал как правильно сбрасывать кэш диска, как не знали и некоторые малоиспользуемые файловые системы.

Если верить статье [Ensuring data reaches disk](https://lwn.net/Articles/457667/), то начиная с версии 2.6.35 ext3, ext4, xfs и btrfs могут быть смонтированы с флагом `barrier`, чтобы включить барьеры (сброс дискового кэша).
По крайней мере, в man странице для [mount](https://man.linuxexplore.com/htmlman8/mount.8.html) эти файловые системы имеют флаг `barrier`.

Я немного поискал в [исходниках Linux](https://github.com/torvalds/linux/tree/master/fs) файловые системы, которые не умеют выполнять fsync, но пришел к выводу, что `fsync` не реализован только в readonly файловых системах (например, [efs](https://github.com/torvalds/linux/blob/67be068d31d423b857ffd8c34dbcc093f8dfff76/fs/efs/dir.c#L13) и [isofs](https://github.com/torvalds/linux/blob/67be068d31d423b857ffd8c34dbcc093f8dfff76/fs/isofs/dir.c#L268) не регистрируют `fsync`).

## Гарантии записи

В конце хочется поговорить о том, какие гарантии записи дают разные устройства. 
Под этим я сейчас подразумеваю 2 вещи:
- Атомарность записи
- Powersafe Overwrite

### Атомарность записи

Чтение и запись на диск производится не по 1 байту за раз, а блоками, по несколько.

Для HDD единица чтения и записи - сектор.
Но для SSD единицы чтения и записи разные. 
Единица чтения - страница, а записи - блок.

Но сейчас больше интересна атомарность записи, поэтому внимание обращать будем на единицу записи.
Согласно этому ответу [на StackOverflow](https://stackoverflow.com/a/61832882) - запись скорее всего (likely) атомарна, но при условии, что:
- Контроллер диска имеет запасную батарею
- Вендор SCSI диска дает гарантии атомарности записи 
- Для NVMe вызывается функция для атомарной записи

Звучит вполне логично, поэтому примем это за ответ 

### PowerSafe OverWrite

[PowerSafe OverWrite](https://www.sqlite.org/psow.html) - это термин, который используют разработчики SQLite для описания поведения некоторых файловых систем и дисков в случае внезапного отключения электричества.
Заключается оно в следующем:

> When an application writes a range of bytes in a file, no bytes outside of that range will change, even if the write occurs just before a crash or power failure.

Перевод:

> В случае отказа или отключения питания во время записи диапазона байтов в файл, никакие данные за пределами этого диапазона не будут изменены.

Я не нашел никаких исследований или статей, которые бы сказали какая файловая система или диск поддерживает PSOW.
Но скорее всего это из-за того, что термин специфичен для разработчиков SQLite.
Но они говорят, что, в практическом смысле, это свойство означает наличие батареи в накопителе на случай отключения электричества для безопасной записи последних данных секторы.

<spoiler title="Атомарность и PSOW - не пересекаются">

Атомарность и PowerSafe OverWrite - это разные характеристики и одно не является частным случаем другого.

Для примера рассмотрим такую ситуацию - мы хотим перезаписать участок файла и в момент перезаписи отключилось электричество.
В зависимости от различных комбинаций, последствия будут разными.

Представим, что у нас есть 3 сектора, заполненных 0, и хотим перезаписать определенный диапазон единицами, причем этот диапазон затрагиваетс.
Изобразим следующим образом.

```text
              А         Б         В
Секторы: |000000000|000000000|000000000|        
Запись:       |------------------|
```

Тогда в зависимости от свойств последствия могут быть следующими:

1. Atomic + PSOW

   Каждый сектор содержит либо старые данные, либо полностью обновленные данные (биты были перезаписаны).

   Возможные ситуации:
   ```text
   А: |000011111|000000000|000000000|
           |------------------|
   
   Б: |000000000|111111111|000000000|
           |------------------|
   
   В: |000000000|000000000|111100000|
           |------------------|
   ```

2. !Atomic + PSOW

   Представим, что при записи в сектор А произошел сбой. 
   Тогда в диапазоне от начала записи и до конца возможно любое состояния, т.е. будут смешаны старые и новые данные, но биты за пределами этого сектора затронуты не будут.

   Тогда возможны такие ситуации:
   ```text
   А: |000011010|000000000|000000000|
           |------------------|
   
   Б: |000000000|110011010|000000000|
           |------------------|
   
   В: |000000000|000000000|001000000|
           |------------------|
   ```
   
   Главное заметим, что данные за пределами этого диапазона не были изменены.

3. Atomic + !PSOW
  
   Дела становятся интереснее, когда запись в сектор атомарна, но PSOW гарантировать не можем.
   Пример подобного поведения привели разработчики SQLite: при перезаписи участка файла ОС считывает весь сектор, изменяет в нем нужные байты, записывает на диск (Read Modify Write) и в момент записи происходит отключение электричества.
   Данные были записаны только частично, ECC не был обновлен и при запуске диск обнаруживает, что сектор некорректный и зануляет его.
   Хоть запись и атомарна, но данные за пределами диапазона были изменены.

   В нашем примере это можно представить следующим образом (единицы означают чистые страницы):
   ```text
   А: |111111111|000000000|000000000|
           |------------------|
   
   Б: |000000000|111111111|000000000|
           |------------------|
   
   В: |000000000|000000000|111111111|
           |------------------|
   ```

4. !Atomic + !PSOW
   
   Это последний и самый страшный пример. 
   В этом случае, мы никаких гарантий не даем и абсолютно любое изменение единственного байта может привести к инвалидации всего сектора.
   
   Картина в данном случае похожа на 3 случай.

В репозитории [hashicorp/raft-wal](https://github.com/hashicorp/raft-wal) имеется README, в котором описаны некоторые приложения и их предположения относительно гарантий. 

| Приложение                                                                                            | Атомарность | PowerSafe OverWrite |
|-------------------------------------------------------------------------------------------------------|-------------|---------------------|
| [SQLite](https://sqlite.org/atomiccommit.html#_hardware_assumptions)                                  | -           | + (начиная с 3.7.9) |
| [Hashicorp](https://github.com/hashicorp/raft-wal/tree/main?tab=readme-ov-file#our-assumptions)       | -           | +                   |
| [Etcd/wal](https://github.com/hashicorp/raft-wal/tree/main?tab=readme-ov-file#user-content-etcd-wal)  | +           | +                   |
| [LMDB](https://github.com/hashicorp/raft-wal/tree/main?tab=readme-ov-file#user-content-lmdb)          | +           | -                   |
| [BoltDB](https://github.com/hashicorp/raft-wal/tree/main?tab=readme-ov-file#user-content-rocksdb-wal) | +           | +                   |

</spoiler>

### TODO: секции "что делать" удалить

На этом можно было бы и закончить, но мы пропустили один довольно важный слой - среда выполнения.

# Рантайм

В самом начале мы перешли от приложения сразу к операционной системе. 
Но между ними может лежать еще один слой - рантайм:
- Среда выполнения - Node.js, .NET, JVM
- Интерпретаторы - Python, Ruby

Если при работе в C/C++ можно сразу вызвать `fsync`, то для других яп надо учитывать различные аспекты рантайма.

Сейчас поговорим про `fsync`, так как он необходим для подтверждения сохранности данных.

В java имеется метод `force(true)` для этого. В документации написано, что этот метод

> Forces any updates to this channel's file to be written to the storage device that contains it.

То есть напрямую `fsync` не вызывается, мы полагаемся на интерфейс, который среда предлагает.

То же самое можем увидеть в .NET - у класса `FileStream` есть перегруженный метод `Flush(bool flushToDisk)`.
Если ему передать значение `true`, то все данные будут записаны на диск:

> Use this overload when you want to ensure that all buffered data in intermediate file buffers is written to disk.
> When you call the Flush method, the operating system I/O buffer is also flushed.

Но стоит заметить, что ничего про `fsync` не сказано.
Да, это платформозависимая деталь реализации, но если мы хотим точно убедиться в сохранности лучше проверить.

Я решил проверить, как себя ведет вызов этого метода. 
Сначала поискал в исходниках и нашел следующую цепочку вызовов:

<spoiler title="Цепочка вызовов">

```cs
public class FileStream
{
    private readonly FileStreamStrategy _strategy;
    
    // https://github.com/dotnet/runtime/blob/da781b3aab1bc30793812bced4a6b64d2df31a9f/src/libraries/System.Private.CoreLib/src/System/IO/FileStream.cs#L389
    public virtual void Flush(bool flushToDisk)
    {
        if (_strategy.IsClosed)
        {
            ThrowHelper.ThrowObjectDisposedException_FileClosed();
        }

        _strategy.Flush(flushToDisk);
    }
}

internal abstract class OSFileStreamStrategy : FileStreamStrategy
{
    // https://github.com/dotnet/runtime/blob/da781b3aab1bc30793812bced4a6b64d2df31a9f/src/libraries/System.Private.CoreLib/src/System/IO/Strategies/OSFileStreamStrategy.cs#L137
    internal sealed override void Flush(bool flushToDisk)
    {
        if (flushToDisk && CanWrite)
        {
            FileStreamHelpers.FlushToDisk(_fileHandle);
        }
    }
}

internal static partial class FileStreamHelpers
{
    // https://github.com/dotnet/runtime/blob/da781b3aab1bc30793812bced4a6b64d2df31a9f/src/libraries/System.Private.CoreLib/src/System/IO/Strategies/FileStreamHelpers.Unix.cs#L40
    internal static void FlushToDisk(SafeFileHandle handle)
    {
        if (Interop.Sys.FSync(handle) < 0)
        {
            Interop.ErrorInfo errorInfo = Interop.Sys.GetLastErrorInfo();
            switch (errorInfo.Error)
            {
                case Interop.Error.EROFS:
                case Interop.Error.EINVAL:
                case Interop.Error.ENOTSUP:
                    // Ignore failures for special files that don't support synchronization.
                    // In such cases there's nothing to flush.
                    break;
                default:
                    throw Interop.GetExceptionForIoErrno(errorInfo, handle.Path);
            }
        }
    }
}

internal static partial class Interop
{
    internal static partial class Sys
    {
        // https://github.com/dotnet/runtime/blob/da781b3aab1bc30793812bced4a6b64d2df31a9f/src/libraries/Common/src/Interop/Unix/System.Native/Interop.FSync.cs#L11
        [LibraryImport(Libraries.SystemNative, EntryPoint = "SystemNative_FSync", SetLastError = true)]
        internal static partial int FSync(SafeFileHandle fd);
    }
}

// https://github.com/dotnet/runtime/blob/da781b3aab1bc30793812bced4a6b64d2df31a9f/src/native/libs/System.Native/pal_io.c#L736
int32_t SystemNative_FSync(intptr_t fd)
{
    int fileDescriptor = ToFileDescriptor(fd);

    int32_t result;
    while ((result =
#if defined(TARGET_OSX) && HAVE_F_FULLFSYNC
    fcntl(fileDescriptor, F_FULLFSYNC)
#else
    fsync(fileDescriptor)
#endif
    < 0) && errno == EINTR);
    return result;
}
```

</spoiler>

То есть, при указании `true`, должен произойти вызов `fsync`.
Дальше я захотел проверить это в реальности. 
Для этого написал следующий код и отследил его выполнение с помощью `strace`.

### TODO: ссылка на код

```cs
using var file = new FileStream("sample.txt", FileMode.OpenOrCreate);
file.Write("hello, world"u8);
file.Flush(true);
```

Вот часть вывода `strace` с открытия и до закрытия файла.

```shell
openat(AT_FDCWD, "/path/sample.txt", O_RDWR|O_CREAT|O_CLOEXEC, 0666) = 19
lseek(19, 0, SEEK_CUR)                  = 0
pwrite64(19, "hello, world", 12, 0)     = 12
fsync(19)                               = 0
flock(19, LOCK_UN)                      = 0
close(19)                               = 0
```

По шагам:
1. `openat` - Открыт файл с дескриптором 19
2. `lseek` - Указатель смещен в самое начала
3. `pwrite64` - Записаны наши данные
4. `fsync(19)` - Вызов `fsync`, сброс данных на диск
5. `close(19` - Закрыли файл

Вот и хорошо - `fsync` вызывается. 
Но сейчас у меня версия .NET - 8.0.1.
Мне стало интересно, а что будет на других версиях.
Я выставил версию .NET 7, скомпилировал с теми же параметрами и запустил `strace` уже для него:

```shell
openat(AT_FDCWD, "/path/sample.txt", O_RDWR|O_CREAT|O_CLOEXEC, 0666) = 19
lseek(19, 0, SEEK_CUR)                  = 0
pwrite64(19, "hello, world", 12, 0)     = 12
flock(19, LOCK_UN)                      = 0
close(19)                               = 0
```

В последних строках нет `fsync`!
Более того, если вызвать `Flush(true)` еще раз, то он появится:
```shell
openat(AT_FDCWD, "/home/ash-blade/Projects/habr-posts/file-write/src/FileWrite.FsyncCheckNet7/bin/Release/net7.0/sample.txt", O_RDWR|O_CREAT|O_CLOEXEC, 0666) = 19
lseek(19, 0, SEEK_CUR)                  = 0
pwrite64(19, "hello, world", 12, 0)     = 12
fsync(19)                               = 0
flock(19, LOCK_UN)                      = 0
close(19)                               = 0
```

В итоге, я пришел к выводу, что первый вызов `Flush(true)` по каким-то причинам игнорируется, а последующие успешно вызывают `fsync`.

Также стоит поговорить о возможностях, предоставляемых самим языком.
Например, `fsync` может (и должен) вызывать на директориях, чтобы убедиться в создании или удалении файлов, поэтому нам необходимо получить файловый дескриптор директории.

Тут я опять пожалуюсь на C# - понятия дескриптор для директории тут нет. 
А получить дескриптор директории обычными способами нельзя - у класса `Directory` нет метода `Open` или какого-нибудь наподобие `Sync`, а если передать в `FileStream` директорию, даже с указанием ReadOnly режима, то возникнет исключение `UnauthorizedAccessException`.

Я нашел обходной путь: с помощью импорта функции `open` получаем дескриптор директории, а после создаем `SafeFileHandle`, в который его и передаем. 
В этом случае, исключений нет и `fsync` вызывается.

```cs
var directory = Directory.CreateDirectory("sample-directory");
const int directoryFlags = 65536; // O_DIRECTORY | O_RDONLY
var handle = Open(directory.FullName, directoryFlags); 
using var stream = new FileStream(new SafeFileHandle(handle, true), FileAccess.ReadWrite);
stream.Flush(true);

[DllImport("libc", EntryPoint = "open")]
static extern nint Open(string path, int flags);
```

И вот `strace`

```shell
openat(AT_FDCWD, "/path/sample-directory", O_RDONLY|O_DIRECTORY) = 19
lseek(19, 0, SEEK_CUR)                  = 0
lseek(19, 0, SEEK_CUR)                  = 0
fsync(19)                               = 0
close(19)                               = 0
```

# Рецепты файловой записи

Тут пример undo/redo логов, атомарного rename

Вот тут пример с undo log и мой пример с рафтом и т.д.

## Как создавать или обновить инициализированный файл

Стоит еще и рассказать про один интересный паттерн создания файлов.

Представим, что нам нужно создать новый файл и при этом его нужно инициализировать каким-то содержимым.

Первая идея - создать новый файл и записать в него нужные значения.
Проблема здесь в неатомарности операции: создание файла и запись в него данных. Машина может отключиться прямо в середине записи в файл и тогда в файле может быть только половина содержимого.
Это не проблема, если файл новый, - просто начать заново. Но что если нам нужно перезаписать существующий файл?

Чтобы понять как это решить, посмотрим на часть документации к `rename`:

> If `newpath` already exists, it will be atomically replaced, so that there is no point at which another process attempting to access `newpath` will find it missing.

Т.е. `rename` атомарная операция и если мы хотим изменить часть содержимого файла, либо создать готовый, то:
1. Создаем временный файл
2. Записываем в него данные
3. Переименовываем файл в целевой - `rename`
4. Вызываем `fsync` для директории (не забываем)

Согласно этому [исследованию](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-pillai.pdf), такой подход используется в Git, Mercurial, LevelDB, HSQLDB, VMWare, HDFS.
Я нашел такой подход в [etcd](https://github.com/etcd-io/etcd/blob/e54bd67554fa2d57e882a6cc949cc90624fecb29/server/storage/wal/wal.go#L733) при создании нового сегмента лога:
```go
// cut closes current file written and creates a new one ready to append.
// cut first creates a temp wal file and writes necessary headers into it.
// Then cut atomically rename temp wal file to a wal file.
func (w *WAL) cut() error {
    // ... Закрытие старого конца 
	
	// create a temp wal file with name sequence + 1, or truncate the existing one
	newTail, err := w.fp.Open()

	// ... Записываем данные в новый сегмент - инициализируем

    // ... Атомарно переименовываем готовый файл сегмента
	// atomically move temp wal file to wal file
	if err = w.sync(); err != nil {
		return err
	}
	
	if err = os.Rename(newTail.Name(), fpath); err != nil {
		return err
	}
	
	// ... Вызываем fsync для директории с сегментами лога
	if err = fileutil.Fsync(w.dirFile); err != nil {
		return err
	}
    
    // ... Обновление внутреннего состояния приложения
}
```

В документации `rename` также прописано, что возможно существование окна, когда старый и новый пути указывают на файл, который собираемся переименовать.
Но так как мы своего добились - атомарно обновили содержимое файла, то в случае отказа просто останется старый временный файл (еще один hardlink), который просто надо удалить.

## Перезапись участка файла

## TODO: еще найти

# Мой случай

Описать что использую сегментированный лог
Атомарность записи в сектор не волнует
Так как использую рафт, то 

# Заключение

Вроде бы простая задача - записать данные на диск, но слишком много подводных камней. 
Чтобы быть точно уверенным нужно учитывать множество факторов.

https://lwn.net/Articles/457667/